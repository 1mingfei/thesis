\section{Fitting Vacancy Migration Barriers using Neural Network}
\label{Chap:Al/Vac:section:NN}

The calculations that provided the energetics used to evaluate energy differences and activation barriers before and after vacancy jump in Al-Mg-Zn alloys were implemented by \ac{DFT} together with \acf{CI-NEB} in the \ac{VASP} software with VTST package from Henkelman's group\cite{henkelman2000climbing,henkelman2000improved}. The \acf{NEB} is a general method for finding saddle points and minimum energy paths between the atomic configurations of the known initial and final states. This method works by optimizing a certain number of intermediate images along the reaction path. Each image finds the lowest possible energy while maintaining linear spline spacing to neighboring images. This is done by adding artificial spring forces along with the band between images and by projecting out the force component parallel to the band due to the interatomic potential. The \ac{CI-NEB} method modifies the \ac{NEB} method in a way that the highest-energy image is pushed up to the saddle point by trying to maximize its energy along with the band and minimize in all other directions. When the algorithm converges, the highest-energy image is at the exact saddle point, so a smaller number of intermediate images is needed in \ac{CI-NEB} than \ac{NEB}. With this setup, we can use high-throughput calculations to extract forward/backward activation barriers by calculating the energy differences between the image of the highest energy and the initial/final image.

\begingroup
\begin{figure}[!ht]
  \centering
  \subfigure[]{\includegraphics[width=0.65\linewidth]{Chap5/plots/kpoints_converge.pdf}}
  \subfigure[]{\includegraphics[width=0.65\linewidth]{Chap5/plots/kpoints_speed.pdf}}
\caption[Plot of k-point grids convergence and efficiency test on vacancy migration barrier.]{Plot of k-point grids convergence and efficiency test on vacancy migration barrier. (a) Vacancy migration barriers along reaction coordinates of (1$\times$1$\times$1), (2$\times$2$\times$2), and (4$\times$4$\times$4) k-point grid setups are tested. (b) The total time elapsed using (1$\times$1$\times$1), (2$\times$2$\times$2), and (4$\times$4$\times$4) k-point grid setups are plotted. A 256-atom randomly generated Al-Mg-Zn configuration is used for this test.}
\label{Chap:Al/Vac:fig:kpoints}
\end{figure}
\endgroup

Pseudopotentials based on the \ac{PAW} method were employed for the elemental constituents with the \ac{GGA} of \ac{PBE} for the exchange-correlation energy functional, $\mu_{xc}$, with the interpolation formula of Vosko et al. \cite{vosko1980accurate}. Using plane-wave cutoff energy of 450.0 eV, the total energy for all models of initial and final images was converged to $10^{âˆ’7}$ eV/cell. The reciprocal space of bulk supercells was sampled with (2$\times$2$\times$2) k-point grids. Each grid was generated using the Monkhorst-Pack scheme \cite{monkhorst1976special}. In Figure \ref{Chap:Al/Vac:fig:kpoints}, we plotted the vacancy migration barrier of a 256-atom randomly generated Al-Mg-Zn configuration using different k-point grids. In subplot (a), (2$\times$2$\times$2) and (4$\times$4$\times$4) k-point grids do not have a significant difference on reaction coordinates and migration barrier. However, running on a 120-core machine, (4$\times$4$\times$4) k-point takes 5 times longer simulation time. Considering the efficiency, we choose (2$\times$2$\times$2) k-points with almost no accuracy compromise. A (4$\times$4$\times$4) conventional fcc supercell with a single vacancy embedded was used for these calculations. For activation barrier calculations, 5 images between relaxed initial and final images were used. A spring constant was set to 5 $\text{eV}/\angstrom^2$. The force convergence criteria for all models was set to be less than 0.05 $\text{eV}/\angstrom$. The force-based quick-min optimizer was used to make \ac{NEB} calculations stable for high local concentration cases\cite{sheppard2008optimization}.


We calculated lattice constant for Al-Mg-Zn alloys based on the chemical composition of  7075 Al alloy. A (4$\times$4$\times$4) conventional supercell with 256 atoms was used to generate random lattice occupations by solute atoms using \acfp{SQS} method \cite{zunger1990special}. The types of 256 atoms were chosen to be 244 Al atoms, 7 Mg atoms, and 5 Zn atoms, which is within the concentration range of Al 7075 alloy. The obtained lattice constant is 4.05838 \angstrom, which is roughly equal to the lattice constant of pure Al (4.041\angstrom from the same DFT setup parameters) considering that Zn and Mg atomic weights are not high\cite{haas2009calculation}.


Our \ac{NN} model training set contains mainly from the calculated vacancy migration barriers in two different types of supercells, as shown in Figure \ref{Chap:Al/Vac:fig:atomic_illu}: 1) (4$\times$4$\times$4) randomly generated solid-solution structures with different local concentrations around the jump pair, which is defined by the two lattice sites for the vacancy before and after the migration event. 2) (2$\times$2$\times$2) ordered structures embedded in (4$\times$4$\times$4) pure Al. The first training set is good for simulating vacancy migration of a very early stage, during which Al alloy is still in the solid-solution state. The second training set is designed to accurately describe the behavior of vacancy migrations across/along the boundary between solid-solution Al and ordered clusters/precipitates, and moving inside the ordered phases. The atomic structures of ordered phases are randomly chosen from the proposed GP zone structures from \cite{berg2001gp} and low energy ordered $\text{L1}_\text{0}, \text{L1}_\text{2}, \text{L1}_\text{0}^*, \text{W2}, \text{CH}, \text{L1}_\text{2}^*, \text{Z1}$ structures of Au-Fe from \cite{zhuravlev2017phase} with random perturbations in lattice occupations. The atomic structures were generated using our in-house code KNN2\cite{Zhang2020KNN2}. 

\begingroup
\begin{figure}[!ht]
  \centering
  \subfigure[]{\includegraphics[width=0.49\linewidth]{Chap5/plots/ss_atomic.jpg}}
  \subfigure[]{\includegraphics[width=0.49\linewidth]{Chap5/plots/ordered_atomic.jpg}}
\caption[Atomistic pictures of (4$\times$4$\times$4) supercells containing 256 atoms.]{Atomistic pictures of (4$\times$4$\times$4) supercells containing 256 atoms. (a) One typical (4$\times$4$\times$4) randomly generated solid-solution structure. (b) One typical (2$\times$2$\times$2) randomly chosen ordered structures embedded in (4$\times$4$\times$4) pure Al. Light green, dark green, and red atoms are Al, Mg, and Zn, respectively. The small gray atom represents the location of vacancy.}
\label{Chap:Al/Vac:fig:atomic_illu}
\end{figure}
\endgroup


Many different machine learning/deep learning models are widely used to study materials science problems\cite{bartok2010gaussian,behler2011atom,szlachta2014accuracy,artrith2016implementation,mehta2014exact,artrith2017efficient}. For our particular case, the feed-forward Artificial Neural Network (ANN) is chosen, as it provides a general frame to map non-linear input (atomic species of neighboring environment) to a continuous regressand (vacancy migration barrier). It is well known that a sufficiently large number of hidden neurons can approximate any continuous multivariate function\cite{hornik1989multilayer}. This property gives us the most expandability of this framework when the system needs to go even further complicated in terms of the number of species considered. 

\begin{table}[!htbp]
\centering
\caption[Atom species encoding map for the \acf{NN} input layer.]{Atom species encoding map for the \acf{NN} input layer. Here, ``Vac'' represents vacancies.}
\label{Chap:Al/Vac:tab:mapping}
\begin{tabular}{cc}
\\
\hline
\hline
Species & Encoding  \\ \hline
Al & 1.0 \\
Mg & 2.0 \\
Zn & 3.0 \\
Vac & 4.0 \\
\hline
\hline
\end{tabular}
\end{table}

\begingroup
\begin{figure}[!ht]
  \centering
  \subfigure{\includegraphics[width=0.5\linewidth]{Chap5/plots/2nn.png}}
\caption[Illustration of atomic structures of the $\text{1}^{st}$ and $\text{2}^{nd}$ nearest neighbors surrounding the jumping pairs.]{Illustration of atomic structures of the $\text{1}^{st}$ and $\text{2}^{nd}$ nearest neighbors surrounding the jumping pairs (the lattice sites of the vacancy before and after a migration event). The 26 neighboring atoms have 2-fold symmetry, mirror symmetry along Y-Z plane, and mirror symmetry along the X-Y plane.}
\label{Chap:Al/Vac:fig:2nn}
\end{figure}
\endgroup

The output layer of our \ac{NN} model predicts vacancy migration barriers in a 1-D continuous space. Like the Ising model, the input layer was chosen to be 27 discrete numbers representing atom species that occupy all the $\text{1}^{st}$ and $\text{2}^{nd}$-nearest-neighbor lattice sites of the vacancy before and after the migration events (the jump pair), as shown in Table \ref{Chap:Al/Vac:tab:mapping}. Among the 27 numbers, the first one indicates the type of atom that will be swapped with the vacancy. The rest 26 numbers represent the neighboring atoms of the jump pair up to their $\text{2}^{nd}$ nearest neighbors (each lattice site of the jump pair has 12  $\text{1}^{st}$ nearest neighbors and 6 $\text{2}^{nd}$ nearest neighbors. In addition, both of them share 10 neighbors in common. So 26 = (12 + 6)$\times$2-10). All these 26 atoms as shown in Figure \ref{Chap:Al/Vac:fig:2nn}, where the X, Y, and Z axes are defined as the following. In principle, X, Y, and Z directions can be any orthogonal vectors, as long as they are consistent during the training and serving stages of the simulation. Here, we choose Y as a $\langle$110$\rangle$ vector along the vacancy jumping direction; X is along another $\langle$1$\bar{1}$0$\rangle$ vector perpendicular to Y; Z is along the $\langle$001$\rangle$ vector perpendicular to both X and Y axes. Beside the first number for the swapped site for the vacancy jump, all the left 26 numbers are arranged in their geographical order, so their position will always respond to the same input neuron in the \ac{NN} architecture. In addition, this cluster of 26 neighboring atoms also has 2-fold symmetry  ($R_{2 fold} = \begin{bmatrix} 1 & -1 & 0 \\-1 & -1 & 0 \\0 & 0 & -1 \\\end{bmatrix}$), mirror symmetry along Y-Z plane ($R_{x mirror} = \begin{bmatrix} -1 & 1 & 0 \\1 & 1 & 0 \\0 & 0 & 1 \\\end{bmatrix}$), and mirror symmetry along the X-Y plane($R_{z mirror} = \begin{bmatrix} 1 & -1 & 0 \\1 & 1 & 0 \\0 & 0 & -1 \\\end{bmatrix}$). Therefore, for one jumping event, the vacancy migration barrier is taken as the average of the four symmetric configurations of the 26 neighboring atoms to have a rotation-invariant prediction. Details of this method is described in Algorithm \ref{algo:encode}. By using several layers of hidden neurons between 27 input and the final output, the contribution of pair-wise and triple-wise interactions can be learned.

\begin{figure}[!htb]
  \centering
  \begin{minipage}{.7\linewidth}
    \begin{algorithm}[H]
      \caption{Jumping Pair Encoding Algorithm}\label{algo:encode}
      \begin{algorithmic}[1]
        \State set a reference matrix as a 3$\times$3 identity matrix, $m_0$ = $$\begin{bmatrix} 1 & -1 & 0 \\1 & 1 & 0 \\0 & 0 & 1 \\\end{bmatrix}\quad$$.
        
        \State determine the jumping direction $v_0 = r_{atom} - r_{vac}$, where $r$ is the coordinates of atom/vacancy. Noting that in the practice, atoms may need to be wrapped according to periodic boundary conditions.
        \State determine all the 26 neighboring atoms of the ghost atom at the vacancy site and the jumping atom within the $2^{nd}$ nearest neighbor distance cutoff.
        \State randomly pick one $2^{nd}$ nearest neighbor atom with coordinates $r_1$ of the vacancy atom, and calculate $v_1 = r_{1} - r_{vac}$.
        \State calculate $v_2 = v_0 \cdot v_1$.
        \State concatenate matrix $m_1$ = $\begin{bmatrix} v_0 \\v_1 \\v_2 \\\end{bmatrix}$.
        
        \State calculate rotation matrix $R$ for projecting $m_1$ to the reference coordinate $m_0$. 
        \State rotate coordinates of all the 26 atoms by applying the rotation matrix $R$ to project all the atoms need to encode along the same reference directions. The new coordinates denotes as $r'$. 
        \State encode atoms based on the preset mapping of type, and ascending order in X, Y, and Z coordinates accordingly.
        \State apply for the three rotational matrices, 
        $R_{2 fold} = \begin{bmatrix} 1 & -1 & 0 \\
                                      -1 & -1 & 0 \\
                                      0 & 0 & -1 \\
        \end{bmatrix}\quad$
        $R_{x mirror} = \begin{bmatrix} -1 & 1 & 0 \\
                                         1 & 1 & 0 \\
                                         0 & 0 & 1 \\
        \end{bmatrix}\quad$
        $R_{z mirror} = \begin{bmatrix} 1 & -1 & 0 \\
                                        1 & 1 & 0 \\
                                        0 & 0 & -1 \\
        \end{bmatrix}\quad$
        , to the coordinates $r'$ and encode them use the same method mentioned above.
        
      \end{algorithmic}
    \end{algorithm}
  \end{minipage}
\end{figure}



We implemented the \ac{NN} model using Google's TensorFlow \cite{abadi2016tensorflow} with Keras \cite{chollet2015keras}. TensorFlow is an open-source numerical computation framework using data flow graphs with the ability to derive gradients automatically. Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow. The optimized neural network structures were tuned by Talos \cite{Autonomio2019Talos}. The Adam optimizer \cite{kingma2014adam} was used for optimizing weights in the neural network. Xavier uniform initializer \cite{glorot2010understanding} was used to generate the random weights and biases so as to break the symmetry of weighting parameters. The activation functions were chosen to be \acf{ReLu} for all the hidden layers. The activation function for the output layer is chosen to be linear to predict vacancy migration barriers on a continuous scale. Normalization method has also been tested but is not helpful in improving accuracy, as vacancy migration barriers follow a very good normal distribution.  The optimized \ac{NN} architecture can be found in Table \ref{Chap:Al/Vac:tab:NN}. Large neural nets trained on relatively small datasets can overfit the training data. So the dropout as a regularization method is applied to prevent overfitting. This introduces a new hyperparameter, the dropout probability, as listed in Table \ref{Chap:Al/Vac:tab:NN}. Though more complicated \acf{RBM} and many other generative stochastic artificial neural networks can be used as a function approximator, their complexity slows down the \ac{kMC} iterations significantly, so that they are not used here.

\begin{table}[!htbp]
\centering
\caption[Neural network architecture with activation functions and dropout probability of each layer.]{Neural network architecture with activation functions and dropout probability of each layer.}
\label{Chap:Al/Vac:tab:NN}
\begin{tabular}{cccc}
\\
\hline
\hline
Layer & Shape  & Activation  & Dropout Probability\\ 
\hline
Dense & 256    & ReLu       & 0.1                 \\
Dense & 256    & ReLu       & 0.2                 \\
Dense & 128    & ReLu       & 0.0                 \\
Dense & 64     & ReLu       & 0.05                \\
Dense & 64     & ReLu       & 0.0                 \\
Dense & 16     & ReLu       & 0.0                 \\
Dense & 1      & Linear     & N.A.                \\ 
\hline
\hline
\end{tabular}
\end{table}


\begingroup
\begin{figure}[!ht]
  \centering
  \subfigure[]{\includegraphics[width=0.49\linewidth]{Chap5/plots/total.png}}
  \subfigure[]{\includegraphics[width=0.49\linewidth]{Chap5/plots/fit_ordered.png}}
\caption[Predictions accuracy of vacancy migration barriers from neural network surrogate model, compared with DFT calculated results.]{Predictions accuracy of vacancy migration barriers from neural network surrogate model, compared with DFT calculated results. During model training, Equation \ref{Chap:Al/Vac:eq:MSE} was used as the loss function. The orange solid line indicates perfect fitting. Each blue solid dot represents one data point. (a) predictions of all the barriers from DFT training data set. (b) predictions of the barriers in certain ternary ordered structures as the testing data set.}
\label{Chap:Al/Vac:fig:fitting_all}
\end{figure}
\endgroup

\begingroup
\begin{figure}[!ht]
  \centering
  \subfigure[]{\includegraphics[width=0.49\linewidth]{Chap5/plots/total_weighted.png}}
  \subfigure[]{\includegraphics[width=0.49\linewidth]{Chap5/plots/fit_ordered_weighted.png}}
\caption[Predictions accuracy of vacancy migration barriers from neural network model using custom loss function, compared with DFT calculated results.]{Predictions accuracy of vacancy migration barriers from neural network model using custom loss function, compared with DFT calculated results. During model training, Equation \ref{Chap:Al/Vac:eq:custLoss} was used to weigh more on low-energy barriers. The orange solid line indicates perfect fitting. Each blue solid dot represents one data point. The black dashed lines illustrate the confinement introduced by custom loss function with emphasis on low-energy-barrier data points. (a) predictions of all the barriers from DFT training data set. (b) predictions of the barriers in certain ternary ordered structures as the testing data set.}
\label{Chap:Al/Vac:fig:fitting_all_weighted}
\end{figure}
\endgroup

To train the \ac{NN}, two different loss functions are used. The former one is the common \acf{MSE} via:
\begin{equation}
MSE = \frac{1}{N}\sum_{i=1}^{N}({E_a}_{i}^{DFT} - {E_a}_{i}^{NN})^2
\label{Chap:Al/Vac:eq:MSE}
\end{equation}
where $N$ is the number of input data, which can be trained in batch or mini-batch, ${E_a}_i^{DFT}$ is the vacancy migration barrier of $i^{\text{th}}$ data from \ac{DFT} calculation, and ${E_a}_{i}^{NN}$ is from \ac{NN} prediction. As shown in Figure \ref{Chap:Al/Vac:fig:fitting_all}, the model reached a \ac{RMSE} of 0.04313 eV/atom for all the data points. The latter one is a customized loss function as:
\begin{equation}
Loss = \frac{1}{N}\sum_{i=1}^{N}{(1.0 + \alpha (1.5 - {E_a}_{i}^{DFT})({E_a}_{i}^{DFT} - {E_a}_{i}^{NN})^2)}
\label{Chap:Al/Vac:eq:custLoss}
\end{equation}
where $\alpha$ is a tunable knob. A larger $\alpha$ value will weigh more on vacancy migration barriers that are small. In this way, we are able to fit low-energy barriers more accurately, as they are the critical rate-determining steps in a \ac{kMC} simulation. Using this custom loss function, the model reached a \ac{RMSE} of 0.03785 eV/atom for all the data points. Noting that the model trained using the latter loss function is slightly more accurate than the former method because more data points are located far away from 1.5 eV. As we expected, low-energy barriers are predicted with higher accuracy, which can be seen from Figure \ref{Chap:Al/Vac:fig:fitting_all_weighted}.